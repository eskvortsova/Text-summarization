{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "from docx.api import Document\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "from natasha import Doc, Segmenter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Titles reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Путь к файлам с текстами\n",
    "PATH = os.path.abspath('..\\\\rfej_parser\\\\articles\\\\') + '\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sort file names\n",
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "\n",
    "def natural_keys(text):\n",
    "    return [ atoi(c) for c in re.split(r'(\\d+)', text) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_titles() -> list:\n",
    "    \"\"\"Get all titles from .docx to list.\n",
    "    \"\"\"\n",
    "    list_of_titles = []\n",
    "    files = [f for f in glob.glob(PATH + '*.docx')]\n",
    "    # Сортировка названий файлов\n",
    "    files.sort(key=natural_keys)\n",
    "    for f in files:\n",
    "        doc = Document(os.path.abspath(f))\n",
    "        list_of_titles.append(doc.paragraphs[0].text)\n",
    "    return list_of_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Получаем список названий всех статей\n",
    "docs = get_titles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class LSA(object):\n",
    "    \"\"\"stopwords: words to ignore \n",
    "        docs: texts in list\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, stopwords, docs):\n",
    "        self.docs = []\n",
    "        # Cодержит номера документов, в которых встречается каждое слово\n",
    "        self.wdict = {}\n",
    "        # Ключевые слова в матрице\n",
    "        self.dictionary = []\n",
    "        # слова которые исключаем из анализа\n",
    "        self.stopwords = stopwords\n",
    "        # инициализируем сами документы\n",
    "        for doc in docs: \n",
    "            self.add_doc(doc)\n",
    "\n",
    "    def prepare(self):\n",
    "        self.build()\n",
    "        self.calc()\n",
    "        \n",
    "    def tokenizer(self, title: str) -> list:\n",
    "        # Split sentance into tokens and return list.\n",
    "        candidate_pos = ['NOUN', 'ADJF', 'VERB', 'INFN', 'PRTS', 'PRTF']\n",
    "        words_of_sentance = []\n",
    "        doc = Doc(str(title))\n",
    "        doc.segment(Segmenter())  # Разбиваем на токены (слова)\n",
    "        words_raw = [word.text for word in doc.tokens]\n",
    "        for w in words_raw:\n",
    "            if morph.parse(w)[0].tag.POS in candidate_pos: # or str(morph.parse(w)[0].tag) == 'LATN':  \n",
    "                final_word = morph.parse(w)[0].normal_form  # Привести к начальной форме\n",
    "                # Убрать мусор в 1 символ\n",
    "                if len(final_word) != 1:\n",
    "                    words_of_sentance.append(final_word) \n",
    "        return words_of_sentance\n",
    "\n",
    "    def dic(self, word, add = False):\n",
    "        # если слово есть в словаре возвращаем его номер\n",
    "        if word in self.dictionary: \n",
    "            return self.dictionary.index(word)\n",
    "        else: \n",
    "            # если нет и стоит флаг автоматически добавлять, то пополняем словари возвращвем код слова\n",
    "            if add:\n",
    "                self.dictionary.append(word)\n",
    "                return len(self.dictionary) - 1\n",
    "            else: \n",
    "                return None\n",
    "\n",
    "    def add_doc(self, doc: str):\n",
    "        words = [self.dic(word, True) for word in self.tokenizer(doc)]\n",
    "        if not words:\n",
    "            return\n",
    "        self.docs.append(words)\n",
    "        for word in words:\n",
    "            if word in self.stopwords:  \n",
    "                continue\n",
    "            elif word in self.wdict:\n",
    "                self.wdict[word].append(len(self.docs) - 1)\n",
    "            else:                      \n",
    "                self.wdict[word] = [len(self.docs) - 1]\n",
    "\n",
    "    def build(self):\n",
    "        # убираем одиночные слова\n",
    "        self.keys = [k for k in self.wdict.keys() if len(self.wdict[k]) > 0]\n",
    "        self.keys.sort()\n",
    "        # создаём пустую матрицу \n",
    "        self.A = np.zeros([len(self.keys), len(self.docs)])\n",
    "        # наполняем эту матрицу\n",
    "        for i, k in enumerate(self.keys):\n",
    "            for d in self.wdict[k]:\n",
    "                self.A[i,d] += 1\n",
    "\n",
    "    def calc(self):\n",
    "        \"\"\" Вычисление U, S Vt - матриц \"\"\"\n",
    "        self.U, self.S, self.Vt = svd(self.A)\n",
    "\n",
    "    def TFIDF(self):\n",
    "        td = self.A\n",
    "        tf = td / td.sum(axis=0)\n",
    "        idf = np.log(td.shape[1] / np.count_nonzero(td, axis=1)).reshape(-1, 1)\n",
    "        self.A = tf * idf\n",
    "\n",
    "    def dump_src(self):\n",
    "        self.prepare()\n",
    "#         print('Здесь представлен расчет матрицы ')\n",
    "#         for i, row in enumerate(self.A):\n",
    "#             print (self.dictionary[i], row)\n",
    "\n",
    "    def print_svd(self):\n",
    "        self.prepare()\n",
    "        print('Здесь сингулярные значения')\n",
    "        print (self.S)\n",
    "        print('Здесь первые 3 колонки U матрица ')\n",
    "        for i, row in enumerate(self.U):\n",
    "            print (self.dictionary[self.keys[i]], row[0:3])\n",
    "        print ('Здесь первые 3 строчки Vt матрица')\n",
    "        print (-1*self.Vt[0:3, :])\n",
    "\n",
    "    def find(self, word):\n",
    "        word = word.lower()\n",
    "        self.prepare()\n",
    "        idx = self.dic(word)\n",
    "        if not idx:\n",
    "            print ('Слово не встерчается')\n",
    "            return []\n",
    "        if not idx in self.keys:\n",
    "            print ('Слово отброшено, как не имеющее значения (stopwords)')\n",
    "            return []\n",
    "        idx = self.keys.index(idx)\n",
    "        print ('word:', word)\n",
    "        # получаем координаты слова\n",
    "        wx, wy = (-1 * self.U[:, 1:3])[idx]\n",
    "        print ('coordinates: {}, {:0.2f}, {:0.2f}'.format(idx, wx, wy))\n",
    "        arts = []\n",
    "        xx, yy = -1 * self.Vt[1:3, :]\n",
    "        for k, v in enumerate(self.docs):\n",
    "            # получаем координаты документа\n",
    "            ax, ay = xx[k], yy[k]\n",
    "            #вычисляем расстояние между словом и документом\n",
    "            dx, dy = float(wx - ax), float(wy - ay)\n",
    "            arts.append((k, v, ax, ay, math.sqrt(dx * dx + dy * dy)))\n",
    "            # возвращаем отсортированный по расстоянию список\n",
    "        return sorted(arts, key = lambda a: a[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Если необходмо добавить стоп-слова: файл `stopwords.txt`\n",
    "stopwords = set(w.rstrip() for w in open('stopwords.txt', encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Инициализация класса\n",
    "lsa = LSA(stopwords, docs)\n",
    "# Создаем матрицу отсутствия или наличия слова в документе `lsa.A`\n",
    "lsa.dump_src()\n",
    "# Нормализация веса слов в матрице\n",
    "lsa.TFIDF()\n",
    "# SVD\n",
    "lsa.calc()\n",
    "# lsa.print_svd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Слово, по-которому мы будем осуществлять поиск в сис.координат\n",
    "word = \"Россия\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: россия\n",
      "coordinates: 34, -0.00, 0.80\n"
     ]
    }
   ],
   "source": [
    "# Сравнение документов на оси координат и поиск по ним\n",
    "result = lsa.find(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "415 0.7234104344073895 [45, 345, 762, 211, 34, 189, 375, 763, 764] Экономические аспекты военно-технического сотрудничества России со странами Азии и Ближнего Востока*\n",
      "859 0.7260216733421287 [77, 70, 211, 189, 22, 187, 1183, 34, 4] Экономический потенциал Российской Арктики в области природных ресурсов \n",
      "918 0.7268173942477618 [18, 211, 34, 189, 654, 375] Развитие глубокой переработки газа в мировой экономике \n",
      "724 0.7285395842621368 [211, 34, 189, 22, 232, 996, 233, 1077] Сотрудничество России и стран БРИКС в области возобновляемой энергетики: биотопливо\n",
      "65 0.7288315594902037 [94, 211, 52, 114, 34, 139, 226] Цифровое сотрудничество во внешнеэкономической политике России и Республики Корея \n",
      "323 0.7291727113066148 [114, 172, 177, 96, 79, 128, 5, 76, 9, 34] Политика Японии в отношении региональных торговых соглашений и влияние на внешнюю торговлю с Россией\n",
      "1074 0.7299238597978919 [65, 398, 71, 225, 114, 456, 83, 118, 29, 34, 264]  Тенденции и структура взаимной торговли Вьетнама и России\n",
      "968 0.7325685103613243 [173, 945, 789, 34, 17, 1065, 264, 1286, 732] Мировой финансовый рынок на современном этапе: рост инвестиционной активности развивающихся стран\n",
      "914 0.7333015307455695 [79, 114, 34, 177, 1245, 349] Особенности международной торговли легковыми автомобилями\n",
      "572 0.7333025464072903 [110, 65, 111, 96, 114, 169, 34] Возможность использования опыта региональной политики Испании в России\n"
     ]
    }
   ],
   "source": [
    "# Результат, отсортированный по возрастанию расстояния от слова к док-ту в сис.координат\n",
    "# Вывод: номер документа в списке, растояние, документ разложеный на коды, документ\n",
    "for res in result[:10]:\n",
    "    print(res[0], res[4], res[1], docs[res[0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
