{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/home/skvortsova-ev/bert_cls/only_extractive/lib/python3.7/site-packages/')\n",
    "\n",
    "import importlib.util\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "from transformers import AdamW, BertForSequenceClassification\n",
    "from scipy.special import softmax\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU avaliable\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\", 1)\n",
    "    print('GPU avaliable')\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU UNavaliable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spec = importlib.util.spec_from_file_location('BertTokenizer', '/data/bert_data/transformers/transformers/tokenization_bert.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Path_pretrained = {\n",
    "    'vocab': '/data/bert_data/rubert_cased_L-12_H-768_A-12_v1/vocab.txt',\n",
    "    'json': '/data/bert_data/rubert_cased_L-12_H-768_A-12_v1/bert_config.json',\n",
    "    'bin': '/data/bert_data/rubert_cased_L-12_H-768_A-12_v1/pytorch_model.bin',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(Path_pretrained['vocab'], do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(text): \n",
    "    text = re.sub(r'[А-ЯЁ]\\w+ +\\w{1}\\. ?\\w{1}\\.?',' ', text) #fio\n",
    "    text = re.sub(r'[А-ЯЁ][\\w\\.]+ [А-ЯЁ]\\w+ [А-ЯЁ]\\w+','', text) #fio2\n",
    "    text = re.sub(r'[ёЁ]','е', text) #ё\n",
    "    text = re.sub(r'(с|[пд]о)? ?\\d{2}\\.\\d{2}\\.\\d{2,4} ?г?\\.?','', text) #dates\n",
    "    text = re.sub(r'[\\W\\d_]',' ', text) #not string and digits and _\n",
    "    text = re.sub(r'\\s+',' ', text) #whitespaces\n",
    "    return text.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"working_labels_extractive.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['type'] = df['sentenized'].map(lambda x: type(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df[df['type']==str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test = df[df['data_type']=='test']\n",
    "df_train = df[df['data_type']=='train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test['sentenized'] = df_test['sentenized'].map(lambda x: clean_text(x))\n",
    "df_train['sentenized'] = df_train['sentenized'].map(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test['sentenized'] = df_test['sentenized'].map(lambda x: \"[CLS] \" + x + \" [SEP]\")\n",
    "df_train['sentenized'] = df_train['sentenized'].map(lambda x: \"[CLS] \" + x + \" [SEP]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_texts_train = [tokenizer.tokenize(sent) for sent in df_train['sentenized'].to_list()]\n",
    "tokenized_texts_test = [tokenizer.tokenize(sent) for sent in df_test['sentenized'].to_list()]\n",
    "\n",
    "df_train['tokenized'] = tokenized_texts_train\n",
    "df_train['ids'] = df_train['tokenized'].map(lambda x: [tokenizer.convert_tokens_to_ids(y) for y in x])\n",
    "\n",
    "df_test['tokenized'] = tokenized_texts_test\n",
    "df_test['ids'] = df_test['tokenized'].map(lambda x: [tokenizer.convert_tokens_to_ids(y) for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train=df_train.reset_index(drop=True)\n",
    "df_test=df_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def pad_sequences(arr, len_, dtype=np.long):\n",
    "    arr = np.array(arr, dtype=dtype)\n",
    "    if arr.shape[0] >= len_:\n",
    "        return arr[:len_]\n",
    "    return np.concatenate((arr, np.zeros(len_ - arr.shape[0], dtype=dtype)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.hist([len(t) for t in df_train['ids'].to_list()], bins = 80)\n",
    "plt.xlabel('words count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.hist([len(t) for t in df_test['ids'].to_list()], bins = 80)\n",
    "plt.xlabel('words count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train['input_ids'] = df_train['ids'].map(lambda x: pad_sequences(x, 50))\n",
    "df_test['input_ids'] = df_test['ids'].map(lambda x: pad_sequences(x, 50))\n",
    "\n",
    "df_train['attention_masks'] = df_train['input_ids'].map(lambda x: [float(i>0) for i in x])\n",
    "df_test['attention_masks'] = df_test['input_ids'].map(lambda x: [float(i>0) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train.groupby(['km_id']).count()[['filename']].hist(bins = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# разбиваем на бины, в которых равное количество предложений для описания \n",
    "len_violations = df_train.groupby(['violation_index']).count()[['filename']].reset_index()\n",
    "len_violations = len_violations.sort_values(['filename'], ascending = False)\n",
    "lengths = list(set(len_violations['filename'].to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_test.groupby(['violation_index']).count()[['filename']].hist(bins = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "torch.cuda.empty_cache()\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    pretrained_model_name_or_path=Path_pretrained['json'],\n",
    "    state_dict=torch.load(Path_pretrained['bin']),\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "# model.config.output_attentions = True\n",
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = [\"LayerNorm\", \"layer_norm\", \"bias\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.00}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def DrawPlot(train_loss_set, val_loss_set, epoch, step, train_dataloader):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.ylim(bottom=-.15, top=2.5)\n",
    "    plt.plot(train_loss_set)\n",
    "    plt.plot(val_loss_set, linewidth=4)\n",
    "    plt.axhline(0, c='black', linewidth=2)\n",
    "    val_loss_ = np.array(val_loss_set)[np.unique(np.array(val_loss_set), return_index=True)[1][::-1]]\n",
    "    for i in range(epoch):         \n",
    "#         plt.text(len(train_dataloader)*i + len(train_dataloader)//4, -.05, '{:.3f}'.format(val_loss_[i]), fontsize=10, bbox=dict(facecolor='red', alpha=0.5))\n",
    "        plt.text(len(train_dataloader)*i + len(train_dataloader)//4, -.07, '{:.3f}'.format(val_loss_set[i*len(train_dataloader)]), fontsize=10, bbox=dict(facecolor='red', alpha=0.5))        \n",
    "        plt.text(len(train_dataloader)*i + len(train_dataloader)//2, 2, i+1, bbox=dict(facecolor='yellow', alpha=0.3))\n",
    "    for vline in range(epoch+1):\n",
    "        plt.axvline((vline+1)*len(train_dataloader), ymax=max(train_loss_set), color='red')\n",
    "    plt.title(f\"Training loss epoch {epoch+1}, step {step+1}/{len(train_dataloader)}\")\n",
    "    plt.xlabel(\"Batch\")\n",
    "    plt.ylabel(\"Loss\")            \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# разбиваем на бины, в которых равное количество предложений для описания нарушений\n",
    "len_violations_test = df_test.groupby(['violation_index']).count()[['filename']].reset_index()\n",
    "len_violations_test = len_violations_test.sort_values(['filename'], ascending = False)\n",
    "lengths_test = list(set(len_violations_test['filename'].to_list()))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "validation_inputs = torch.tensor(df_test['input_ids'].to_list())\n",
    "validation_labels = torch.tensor(df_test['label'].to_list())\n",
    "validation_masks = torch.tensor(df_test['attention_masks'].to_list())\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "\n",
    "prediction_data = TensorDataset(\n",
    "    validation_inputs,\n",
    "    validation_masks,\n",
    "    validation_labels\n",
    ")\n",
    "\n",
    "prediction_dataloader = DataLoader(\n",
    "    prediction_data, \n",
    "    sampler=SequentialSampler(prediction_data),\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "EPOCH_NUM = 10\n",
    "\n",
    "# Будем сохранять loss во время обучения и рисовать график в режиме реального времени\n",
    "train_loss_set = []\n",
    "val_loss_set = []\n",
    "\n",
    "train_loss = 0\n",
    "val_score = []\n",
    "val_score_f=[]\n",
    "model.train()\n",
    "\n",
    "for epoch in range(EPOCH_NUM):\n",
    "    for num in range(len(lengths)):\n",
    "        BATCH_SIZE = lengths[num]\n",
    "        temp_length_filter = set(len_violations[len_violations['filename']==lengths[num]]['violation_index'].to_list())\n",
    "        temp_df_train = df_train[df_train['violation_index'].isin(temp_length_filter)]\n",
    "        kms = list(set(temp_df_train['violation_index'].to_list()))\n",
    "        for violation in tqdm(range(len(kms))):\n",
    "            df_batch = df_train[df_train['violation_index']==kms[violation]].reset_index(drop = True)\n",
    "            train_inputs = torch.tensor(df_batch['input_ids'].to_list())\n",
    "            train_labels = torch.tensor(df_batch['label'].to_list())\n",
    "            train_masks = torch.tensor(df_batch['attention_masks'].to_list())\n",
    "            \n",
    "            train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "            \n",
    "            train_dataloader = DataLoader(\n",
    "            train_data,\n",
    "            sampler=RandomSampler(train_data),\n",
    "            batch_size=BATCH_SIZE\n",
    "        )\n",
    "            \n",
    "            for step, batch in enumerate(train_dataloader):\n",
    "                # добавляем батч для вычисления на GPU\n",
    "                batch = tuple(t.to(device).to(torch.int64) for t in batch)\n",
    "                # Распаковываем данные из dataloader\n",
    "                b_input_ids, b_input_mask, b_labels = batch\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "\n",
    "                train_loss_set.append(loss[0].item())  \n",
    "\n",
    "                # Backward pass\n",
    "                loss[0].backward()\n",
    "\n",
    "                # Обновляем параметры и делаем шаг используя посчитанные градиенты\n",
    "                optimizer.step()\n",
    "\n",
    "                # Обновляем loss\n",
    "                train_loss += loss[0].item()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "    model.eval()    \n",
    "    val_loss = 0\n",
    "    for num in range(len(lengths_test)):\n",
    "        BATCH_SIZE_test = lengths[num]\n",
    "        temp_length_filter_test = set(len_violations_test[len_violations_test['filename']==lengths_test[num]]['violation_index'].to_list())\n",
    "        temp_df_test = df_test[df_test['violation_index'].isin(temp_length_filter_test)]\n",
    "        kms_test = list(set(temp_df_test['violation_index'].to_list()))\n",
    "        for viol in range(len(kms_test)):\n",
    "        \n",
    "            df_batch_test = df_test[df_test['violation_index']==kms_test[viol]].reset_index(drop = True)\n",
    "            test_inputs = torch.tensor(df_batch_test['input_ids'].to_list())\n",
    "            test_labels = torch.tensor(df_batch_test['label'].to_list())\n",
    "            test_masks = torch.tensor(df_batch_test['attention_masks'].to_list())\n",
    "            \n",
    "            test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "            \n",
    "            test_dataloader = DataLoader(\n",
    "            test_data,\n",
    "            sampler=RandomSampler(test_data),\n",
    "            batch_size=BATCH_SIZE_test)\n",
    "            \n",
    "            for i, val in enumerate(test_dataloader):        \n",
    "                if i > step:\n",
    "                    break        \n",
    "                val = tuple(t.to(device).to(torch.int64) for t in val)\n",
    "                b_input_ids, b_input_mask, b_labels = val\n",
    "                with torch.no_grad():\n",
    "                    logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "                val_loss += logits[0].item()\n",
    "                logits = logits[1].detach().cpu().numpy()\n",
    "                label_ids = b_labels.to('cpu').numpy()\n",
    "                val_score.append(precision_score(np.array(label_ids), np.argmax(logits, axis=1), average='macro'))\n",
    "                val_score_f.append(f1_score(np.array(label_ids), np.argmax(logits, axis=1), average='macro'))\n",
    "                print('EPOCH_NUMBER: ', epoch, 'PRECISION :', precision_score(np.array(label_ids), np.argmax(logits, axis=1), average='macro'))\n",
    "            val_loss_set += [val_loss/(step+1)]*(step+1)\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path= 'bersumext.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
