{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "# for 'sentance_collection' method\n",
    "%run ../rfej_preprocessing/sentence_collection.ipynb\n",
    "# for 'red_txt' method\n",
    "%run ../rfej_preprocessing/find_shortenings.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Путь к файлам с текстами\n",
    "PATH = os.path.abspath('..\\\\rfej_parser\\\\articles\\\\') + '\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from natasha import(\n",
    "    Doc, \n",
    "    Segmenter, \n",
    "    NewsNERTagger, \n",
    "    NewsEmbedding, \n",
    "    MorphVocab, \n",
    "    DatesExtractor, \n",
    "    MoneyExtractor, \n",
    "    AddrExtractor,\n",
    ")\n",
    "segmenter = Segmenter()\n",
    "emb = NewsEmbedding()\n",
    "morph_vocab = MorphVocab()\n",
    "ner_tagger = NewsNERTagger(emb)\n",
    "dates_extractor = DatesExtractor(morph_vocab)\n",
    "money_extractor = MoneyExtractor(morph_vocab)\n",
    "addr_extractor = AddrExtractor(morph_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def articles_to_sentances(path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Split all .txt files to sentances.\n",
    "    \n",
    "    path: path to .txt files\n",
    "    artcls_sentncs: number of text -> {number of sentance -> 'sentance', ...}\n",
    "    \n",
    "    \"\"\"\n",
    "    files = [f for f in glob.glob(PATH + '*.txt') if re.findall(r'_a.txt', f)]\n",
    "    artcls_sentncs = {}\n",
    "    for f in files:\n",
    "        text = read_txt(f)\n",
    "        artcls_sentncs[int(re.findall(r'([\\d]*)_a', f)[0])] = {\n",
    "            key: sentance for key, sentance in enumerate(sentence_collection(text))}\n",
    "    return artcls_sentncs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "# supress 'WARNING:root:Something went wrong while tokenizing'\n",
    "logging.root.level = logging.ERROR\n",
    "\n",
    "# Разбиваем текст на предложения:\n",
    "s = articles_to_sentances(PATH)\n",
    "\n",
    "logging.root.level = logging.WARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_entity_ignored(word: str) -> bool:\n",
    "    \"\"\"Defines do we have to use this entity in algorithm. \n",
    "    If `yes` -> returns True, else -> False.\n",
    "    \"\"\"\n",
    "    entities_ignored = ['LOC', 'ORG', 'PER']\n",
    "    doc = Doc(word)\n",
    "    doc.segment(segmenter)\n",
    "    doc.tag_ner(ner_tagger)\n",
    "    if doc.spans and doc.spans[0].type in entities_ignored:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "def sentances_to_words(sent_dict: dict) -> dict:\n",
    "    \"\"\"\n",
    "    sent_dict: number of text -> {number of sentance -> 'sentance', ...}\n",
    "    artcls_words: number of text -> {number of sentance -> ['word_0', 'word_1', ...], ...}\n",
    "    \n",
    "    \"\"\"\n",
    "    candidate_pos = ['NOUN', 'ADJF', 'VERB', 'INFN', 'PRTS', 'PRTF']\n",
    "    selected_words = []\n",
    "    artcls_words = {}\n",
    "    for artcl in sent_dict:\n",
    "#         if int(artcl) < 10:  # ТЕСТ НА 10 ПЕРВЫХ СТАТЬЯХ!\n",
    "            artcls_words[artcl] = {}\n",
    "            for sentance in sent_dict[artcl]:\n",
    "                words_of_sentance = []\n",
    "                doc = Doc(sent_dict[artcl][sentance])\n",
    "                doc.segment(segmenter)\n",
    "                words_raw = [word.text for word in doc.tokens]\n",
    "                for w in words_raw:\n",
    "                    if morph.parse(w)[0].tag.POS in candidate_pos and not is_entity_ignored(w):\n",
    "                        final_word = morph.parse(w)[0].normal_form  # привести к начальной форме\n",
    "                        # Убрать мусор в 1 символ\n",
    "                        if len(final_word) != 1:\n",
    "                            words_of_sentance.append(final_word) \n",
    "                words_of_sentance = set(words_of_sentance)  # Убираем поторяющиеся слова\n",
    "                if len(words_of_sentance) > 2:  # Порог для длины предложений  - 3 слова\n",
    "                    artcls_words[artcl][sentance] = [word for word in words_of_sentance]\n",
    "    return artcls_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разбиваем предложения на слова\n",
    "w = sentances_to_words(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from textdistance import Sorensen\n",
    "\n",
    "# Вычисление схожести предложений по коэффициенту Сёренсена.\n",
    "res_scores = {}\n",
    "srnsn = Sorensen()\n",
    "for artcl in w:\n",
    "    nbrs = []\n",
    "    for sent_nb in range(len(w[artcl])):\n",
    "        if sent_nb in w[artcl].keys():\n",
    "            nbrs.append(sent_nb)\n",
    "    pairs = combinations(nbrs, 2)\n",
    "    scores = [(i, j, srnsn.similarity(w[artcl][i], w[artcl][j])) for i, j in pairs]\n",
    "    # Убираем не связанные пары (0.0)\n",
    "    res_scores[artcl] = [el for el in filter(lambda x: x[2], scores)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "# Создаем граф: вершина - это предложение, ребро - это вес, \n",
    "# равный похожести вершин, которым оно инцидентно \n",
    "def scores_to_graph(w: dict, res_scores: dict) -> dict:\n",
    "    \"\"\"Create graphs from sentances.\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    for article in res_scores:\n",
    "        g = nx.Graph()\n",
    "        g.add_weighted_edges_from(res_scores[article])\n",
    "        # pr: 'sentance number' -> PageRank\n",
    "        pr = nx.pagerank(g)\n",
    "        # Каждый score делим на длину предложения, чтобы она не влияла на ранк\n",
    "        for score in pr:\n",
    "            pr[score] = pr[score] / len(w[article][score])\n",
    "        # Сортируем предложения по PageRank\n",
    "        result[article] = sorted(((i, pr[i]) for i in w[article].keys() if i in pr), \n",
    "                key=lambda x: pr[x[0]], reverse=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ranks = scores_to_graph(w, res_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_average_score(pr):\n",
    "    sum_values = 0\n",
    "    for sentance in pr:\n",
    "        sum_values += sentance[1]\n",
    "    average = sum_values / len(pr)\n",
    "    return average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Считаем порог ранка для каждого текста (средний по тексту)\n",
    "# threshold = {}\n",
    "# for artcl in ranks:\n",
    "#     threshold[artcl] = find_average_score(ranks[artcl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "res_sentances_nb = {}\n",
    "for artcl in ranks:\n",
    "#     number_of_sentances = [sent[0] for sent in filter(lambda x: x[1] >= threshold[artcl], ranks[artcl])]\n",
    "    number_of_sentances = [sent[0] for sent in ranks[artcl]]\n",
    "    NB = ceil(len(number_of_sentances) * 16 / 100) \n",
    "    number_of_sentances = number_of_sentances[:NB]  # БЕРЕМ 20% ПРЕДЛ.\n",
    "    number_of_sentances.sort()\n",
    "    res_sentances_nb[artcl] = number_of_sentances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result (Summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Словарь с суммари: 'number of article' -> ['sentance1, sentance1', ...]\n",
    "sum_textrank = {}\n",
    "\n",
    "for article in w:\n",
    "    sum_textrank[article] = []\n",
    "    for sentance in s[article]:\n",
    "        if sentance in res_sentances_nb[article]:\n",
    "            sum_textrank[article].append(s[article][sentance])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of TextRank summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sum_textrank[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### ROUGE metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'f': 0.13971197701954277,\n",
       "  'p': 0.10240924542658952,\n",
       "  'r': 0.2197598565972747},\n",
       " 'rouge-2': {'f': 0.02415363094347205,\n",
       "  'p': 0.017704073728404116,\n",
       "  'r': 0.037995205082793325},\n",
       " 'rouge-l': {'f': 0.12965111751697095,\n",
       "  'p': 0.09499136299179528,\n",
       "  'r': 0.20413406304525372}}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_textrank # 16% of sentances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'f': 0.14106136813929335,\n",
       "  'p': 0.1232744008653751,\n",
       "  'r': 0.1648467030676705},\n",
       " 'rouge-2': {'f': 0.02208817713994442,\n",
       "  'p': 0.018807195458214607,\n",
       "  'r': 0.026755825390976602},\n",
       " 'rouge-l': {'f': 0.13044844876831158,\n",
       "  'p': 0.11427454976768742,\n",
       "  'r': 0.15195554396767527}}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_textrank_8_sentance  # 8 sentances"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
