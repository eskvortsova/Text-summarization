{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/home/skvortsova-ev/bert_cls/bert_environment/lib/python3.7/site-packages/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "\n",
    "import numpy\n",
    "import torch\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.common.params import Params\n",
    "from allennlp.training.trainer import Trainer\n",
    "from allennlp.data.dataset_readers.dataset_reader import DatasetReader\n",
    "from allennlp.models.model import Model\n",
    "from summarus.readers import act_reader\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from summarus.readers import act_reader\n",
    "from allennlp.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    numpy.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path, train_path, val_path, seed, vocabulary_path, config_path = '/home/skvortsova-ev/notebooks/summarus/pgn_papers/','train_abstractive.csv', 'test_abstractive.csv', 10, '/home/skvortsova-ev/notebooks/summarus/pgn_papers/vocabulary/','/home/skvortsova-ev/notebooks/summarus/tests/configs/papers_pgn.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pretrained_path=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PointerGeneratorNetwork(\n",
      "  (_source_embedder): BasicTextFieldEmbedder(\n",
      "    (token_embedder_tokens): Embedding()\n",
      "  )\n",
      "  (_encoder): LstmSeq2SeqEncoder(\n",
      "    (_module): LSTM(64, 128, batch_first=True, bidirectional=True)\n",
      "  )\n",
      "  (_target_embedder): Embedding()\n",
      "  (_decoder_cell): LSTMCell(320, 256)\n",
      "  (_hidden_projection_layer): Linear(in_features=512, out_features=64, bias=True)\n",
      "  (_output_projection_layer): Linear(in_features=64, out_features=63860, bias=True)\n",
      "  (_p_gen_layer): Linear(in_features=1088, out_features=1, bias=True)\n",
      "  (_attention): BahdanauAttention(\n",
      "    (_decoder_hidden_projection_layer): Linear(in_features=256, out_features=256, bias=False)\n",
      "    (_encoder_outputs_projection_layer): Linear(in_features=256, out_features=256, bias=False)\n",
      "    (_v): Linear(in_features=256, out_features=1, bias=False)\n",
      "    (_coverage_projection_layer): Linear(in_features=1, out_features=256, bias=False)\n",
      "  )\n",
      ")\n",
      "Trainable params count:  22333749\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67e851de321c43d6a7ef9db2823558f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f6aea3bcaad4404954fb70d76fb472c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b83d4dd2fb56445d83844a0e66f7cce5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29b45851a80941239ccfbd439ed9ad5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83c9f9686436479ebbbaf07bd294686f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b228c3643c47437980a9237a409f92f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f403acbae1447218e9dc1255c208148",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38d9f2f1b9b043d0be08bc72ece98491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df26d3750e5140c3aba2c90b8a9bbd69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efbf42fdf45b4ba48a44ba2abf726fca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32469b11b82244e5916f6d0389fb8bc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc29eaca388f482e8007bc52dd47a17e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd57c37a1a09431eac30dd3b59c791e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fd9a4509c204b0ea9b2395f7b5c1f67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'best_epoch': 3,\n",
       " 'peak_cpu_memory_MB': 1996.304,\n",
       " 'peak_gpu_0_memory_MB': 9901,\n",
       " 'peak_gpu_1_memory_MB': 2,\n",
       " 'training_duration': '1:13:31.344790',\n",
       " 'training_start_epoch': 0,\n",
       " 'training_epochs': 5,\n",
       " 'epoch': 5,\n",
       " 'training_coverage_loss': 0.8514687808399851,\n",
       " 'training_p_gen': 0.8057389709343253,\n",
       " 'training_loss': 3.848850403222945,\n",
       " 'training_reg_loss': 0.0,\n",
       " 'training_cpu_memory_MB': 1996.304,\n",
       " 'training_gpu_0_memory_MB': 9901,\n",
       " 'training_gpu_1_memory_MB': 2,\n",
       " 'validation_coverage_loss': 0.8563178211082647,\n",
       " 'validation_p_gen': 0.7980787037149283,\n",
       " 'validation_loss': 6.121881720460491,\n",
       " 'validation_reg_loss': 0.0,\n",
       " 'best_validation_coverage_loss': 0.883503285455115,\n",
       " 'best_validation_p_gen': 0.760794092700084,\n",
       " 'best_validation_loss': 6.111411993886217,\n",
       " 'best_validation_reg_loss': 0.0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert os.path.isdir(model_path), \"Model directory does not exist\"\n",
    "set_seed(seed)\n",
    "\n",
    "config_path = config_path or os.path.join(model_path, \"config.json\")\n",
    "assert os.path.isfile(config_path), \"Config file does not exist\"\n",
    "params = Params.from_file(config_path)\n",
    "\n",
    "vocabulary_path = vocabulary_path or os.path.join(model_path, \"vocabulary\")\n",
    "assert os.path.exists(vocabulary_path), \"Vocabulary is not ready, do not forget to run preprocess.py first\"\n",
    "vocabulary = Vocabulary.from_files(vocabulary_path)\n",
    "\n",
    "reader_params = params.duplicate().pop(\"reader\", default=Params({}))\n",
    "reader = DatasetReader.from_params(reader_params)\n",
    "train_dataset = reader.read(train_path)\n",
    "val_dataset = reader.read(val_path) if val_path else None\n",
    "\n",
    "if not pretrained_path:\n",
    "    model_params = params.pop(\"model\")\n",
    "    model = Model.from_params(model_params, vocab=vocabulary)\n",
    "else:\n",
    "    model = Model.load(params, pretrained_path)\n",
    "print(model)\n",
    "print(\"Trainable params count: \", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "\n",
    "train_dataset.index_with(vocabulary)\n",
    "train_loader = DataLoader(train_dataset, batch_size = 15, shuffle = False)\n",
    "val_dataset.index_with(vocabulary)\n",
    "val_loader = DataLoader(val_dataset, batch_size = 15, shuffle = False)\n",
    "\n",
    "trainer = Trainer.from_params(params.pop('trainer'), model=model, serialization_dir=model_path,\n",
    "                            data_loader=train_loader, validation_data_loader=val_loader, \n",
    "                              patience = 3)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PointerGeneratorNetwork(\n",
       "  (_source_embedder): BasicTextFieldEmbedder(\n",
       "    (token_embedder_tokens): Embedding()\n",
       "  )\n",
       "  (_encoder): LstmSeq2SeqEncoder(\n",
       "    (_module): LSTM(64, 128, batch_first=True, bidirectional=True)\n",
       "  )\n",
       "  (_target_embedder): Embedding()\n",
       "  (_decoder_cell): LSTMCell(320, 256)\n",
       "  (_hidden_projection_layer): Linear(in_features=512, out_features=64, bias=True)\n",
       "  (_output_projection_layer): Linear(in_features=64, out_features=63860, bias=True)\n",
       "  (_p_gen_layer): Linear(in_features=1088, out_features=1, bias=True)\n",
       "  (_attention): BahdanauAttention(\n",
       "    (_decoder_hidden_projection_layer): Linear(in_features=256, out_features=256, bias=False)\n",
       "    (_encoder_outputs_projection_layer): Linear(in_features=256, out_features=256, bias=False)\n",
       "    (_v): Linear(in_features=256, out_features=1, bias=False)\n",
       "    (_coverage_projection_layer): Linear(in_features=1, out_features=256, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import re\n",
    "from typing import Dict\n",
    "\n",
    "from allennlp.common.params import Params\n",
    "from allennlp.models.model import Model\n",
    "from allennlp_models.seq2seq import Seq2SeqPredictor\n",
    "from allennlp.data.dataset_readers.dataset_reader import DatasetReader\n",
    "import torch\n",
    "import nltk\n",
    "import razdel\n",
    "\n",
    "from summarus import *\n",
    "from summarus.util.metrics import print_metrics\n",
    "from summarus.predictors.sentences_tagger_predictor import SentencesTaggerPredictor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def punct_detokenize(text):\n",
    "    text = text.strip()\n",
    "    punctuation = \",.!?:;%\"\n",
    "    closing_punctuation = \")]}\"\n",
    "    opening_punctuation = \"([}\"\n",
    "    for ch in punctuation + closing_punctuation:\n",
    "        text = text.replace(\" \" + ch, ch)\n",
    "    for ch in opening_punctuation:\n",
    "        text = text.replace(ch + \" \", ch)\n",
    "    res = [r'\"\\s[^\"]+\\s\"', r\"'\\s[^']+\\s'\"]\n",
    "    for r in res:\n",
    "        for f in re.findall(r, text, re.U):\n",
    "            text = text.replace(f, f[0] + f[2:-2] + f[-1])\n",
    "    text = text.replace(\"' s\", \"'s\").replace(\" 's\", \"'s\")\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_abs_batches(reader: SummarizationReader,\n",
    "                    test_path: str,\n",
    "                    batch_size: int,\n",
    "                    lowercase: bool = True) -> Dict:\n",
    "    batch = []\n",
    "    for source, target in reader.parse_set(test_path):\n",
    "        source = source.strip()\n",
    "        target = target.strip()\n",
    "        if lowercase:\n",
    "            source = source.lower() if lowercase else source\n",
    "            target = target.lower() if lowercase else target\n",
    "        sample = {\"source\": source, \"target\": target}\n",
    "        batch.append(sample)\n",
    "        if len(batch) == batch_size:\n",
    "            yield batch\n",
    "            batch = []\n",
    "    if batch:\n",
    "        yield batch\n",
    "\n",
    "\n",
    "def get_ext_batches(reader: SummarizationSentencesTaggerReader,\n",
    "                    test_path: str,\n",
    "                    batch_size: int,\n",
    "                    lowercase: bool = True) -> Dict:\n",
    "    batch = []\n",
    "    for _, summary, sentences, tags in reader.parse_set(test_path):\n",
    "        if lowercase:\n",
    "            sentences = [sentence.lower() for sentence in sentences]\n",
    "            summary = summary.lower()\n",
    "        batch.append({\"source_sentences\": sentences, \"sentences_tags\": tags, \"target\": summary})\n",
    "        if len(batch) == batch_size:\n",
    "            yield batch\n",
    "            batch = []\n",
    "    if batch:\n",
    "        yield batch\n",
    "\n",
    "\n",
    "def run_baseline(batch, baseline):\n",
    "    sources = [b.get(\"source\") for b in batch]\n",
    "    targets = [b.get(\"target\") for b in batch]\n",
    "    hyps = []\n",
    "    for source in sources:\n",
    "        source_sentences = nltk.sent_tokenize(source)\n",
    "        if baseline.startswith(\"lead\"):\n",
    "            skip = 0\n",
    "            if \"skip\" in baseline:\n",
    "                skip = int(baseline.split(\"skip\")[-1])\n",
    "                n = int(baseline.split(\"skip\")[0].replace(\"lead\", \"\"))\n",
    "            else:\n",
    "                n = int(baseline.replace(\"lead\", \"\"))\n",
    "            if len(source_sentences) == 1:\n",
    "                skip = 0\n",
    "            hyp = \" \".join(source_sentences[skip:skip+n]).strip()\n",
    "        else:\n",
    "            assert False\n",
    "        hyps.append(hyp)\n",
    "    return targets, hyps\n",
    "\n",
    "\n",
    "def run_abs_model(predictor, batch):\n",
    "    outputs = predictor.predict_batch_json(batch)\n",
    "    targets = [b.get(\"target\") for b in batch]\n",
    "    hyps = []\n",
    "    for output in outputs:\n",
    "        decoded_words = output[\"predicted_tokens\"]\n",
    "        hyp = \" \".join(decoded_words).strip()\n",
    "        hyps.append(hyp)\n",
    "    return targets, hyps\n",
    "\n",
    "\n",
    "def run_ext_model(predictor, batch, top_n=3, border=None):\n",
    "    assert top_n is not None or border is not None\n",
    "    outputs = predictor.predict_batch_json(batch)\n",
    "    targets = [b.get(\"target\") for b in batch]\n",
    "    hyps = []\n",
    "    for sample, output in zip(batch, outputs):\n",
    "        sentences = sample[\"source_sentences\"]\n",
    "        proba = torch.sigmoid(torch.Tensor(output[\"predicted_tags\"])).tolist()\n",
    "        sorted_proba = list(sorted(enumerate(proba), key=lambda x: -x[1]))\n",
    "        if top_n is not None and border is None:\n",
    "            indices = sorted_proba[:top_n]\n",
    "        elif border is not None:\n",
    "            indices = [(i, p) for i, p in sorted_proba if p > border]\n",
    "            if top_n is not None:\n",
    "                indices = indices[:top_n]\n",
    "            if not indices:\n",
    "                indices = sorted_proba[:1]\n",
    "        indices.sort()\n",
    "        hyp = [sentences[i] for i, _ in indices]\n",
    "        hyp = \" \".join(hyp).strip()\n",
    "        hyps.append(hyp)\n",
    "    return targets, hyps\n",
    "\n",
    "\n",
    "def postprocess(ref, hyp, is_subwords=True, is_multiple_ref=False, detokenize_after=False, tokenize_after=True):\n",
    "    hyp = hyp if not is_subwords else \"\".join(hyp.split(\" \")).replace(\"▁\", \" \")\n",
    "    if is_multiple_ref:\n",
    "        reference_sents = ref.split(\" s_s \")\n",
    "        decoded_sents = hyp.split(\"s_s\")\n",
    "        hyp = [w.replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\").strip() for w in decoded_sents]\n",
    "        ref = [w.replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\").strip() for w in reference_sents]\n",
    "        hyp = \" \".join(hyp)\n",
    "        ref = \" \".join(ref)\n",
    "    ref = ref.strip()\n",
    "    hyp = hyp.strip()\n",
    "    if detokenize_after:\n",
    "        hyp = punct_detokenize(hyp)\n",
    "        ref = punct_detokenize(ref)\n",
    "    if tokenize_after:\n",
    "        hyp = \" \".join([token.text for token in razdel.tokenize(hyp)])\n",
    "        #hyp = hyp.replace(\"@ @ UNKNOWN @ @\", \"@@UNKNOWN@@\")\n",
    "        ref = \" \".join([token.text for token in razdel.tokenize(ref)])\n",
    "    return ref, hyp\n",
    "\n",
    "\n",
    "def evaluate(test_path, batch_size, metric, mode,\n",
    "             max_count, report_every, is_multiple_ref=False,\n",
    "             model_path=None, model_config_path=None,\n",
    "             detokenize_after=False, tokenize_after=False, meteor_jar=None):\n",
    "    config_path = model_config_path or os.path.join(model_path, \"config.json\")\n",
    "    params = Params.from_file(config_path)\n",
    "    reader_params = params[\"reader\"].duplicate()\n",
    "    device = 0 if torch.cuda.is_available() else -1\n",
    "    is_subwords = \"tokenizer\" in reader_params and reader_params[\"tokenizer\"][\"type\"] == \"subword\"\n",
    "    reader = DatasetReader.from_params(reader_params)\n",
    "    hyps = []\n",
    "    refs = []\n",
    "\n",
    "    if mode == \"abs\":\n",
    "        model = Model.load(params, model_path, cuda_device=device)\n",
    "        model.eval()\n",
    "        predictor = Seq2SeqPredictor(model, reader)\n",
    "        print(model)\n",
    "        print(\"Trainable params count: \", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "    elif mode == \"ext\":\n",
    "        model = Model.load(params, model_path, cuda_device=device)\n",
    "        model.eval()\n",
    "        predictor = SentencesTaggerPredictor(model, reader)\n",
    "        print(model)\n",
    "        print(\"Trainable params count: \", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "\n",
    "    iter_batches = get_ext_batches if mode == \"ext\" else get_abs_batches\n",
    "    for batch in iter_batches(reader, test_path, batch_size):\n",
    "        if mode == \"abs\":\n",
    "            batch_refs, batch_hyps = run_abs_model(predictor, batch)\n",
    "        elif mode == \"ext\":\n",
    "            batch_refs, batch_hyps = run_ext_model(predictor, batch)\n",
    "        else:\n",
    "            batch_refs, batch_hyps = run_baseline(batch, mode)\n",
    "        for ref, hyp in zip(batch_refs, batch_hyps):\n",
    "            ref, hyp = postprocess(ref, hyp, is_subwords if mode != 'ext' else False, is_multiple_ref, detokenize_after, tokenize_after)\n",
    "            refs.append(ref)\n",
    "            hyps.append(hyp)\n",
    "            if len(hyps) % report_every == 0:\n",
    "                print_metrics(refs, hyps, metric, meteor_jar)\n",
    "            if max_count and len(hyps) >= max_count:\n",
    "                break\n",
    "    print_metrics(refs, hyps, metric, meteor_jar)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
